{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Marital status</th>\n",
       "      <th>Application mode</th>\n",
       "      <th>Application order</th>\n",
       "      <th>Course</th>\n",
       "      <th>Daytime/evening attendance</th>\n",
       "      <th>Previous qualification</th>\n",
       "      <th>Previous qualification (grade)</th>\n",
       "      <th>Nacionality</th>\n",
       "      <th>Mother's qualification</th>\n",
       "      <th>Father's qualification</th>\n",
       "      <th>...</th>\n",
       "      <th>Curricular units 2nd sem (credited)</th>\n",
       "      <th>Curricular units 2nd sem (enrolled)</th>\n",
       "      <th>Curricular units 2nd sem (evaluations)</th>\n",
       "      <th>Curricular units 2nd sem (approved)</th>\n",
       "      <th>Curricular units 2nd sem (grade)</th>\n",
       "      <th>Curricular units 2nd sem (without evaluations)</th>\n",
       "      <th>Unemployment rate</th>\n",
       "      <th>Inflation rate</th>\n",
       "      <th>GDP</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>171</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.74</td>\n",
       "      <td>Dropout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>9254</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>160.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>13.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>13.9</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.79</td>\n",
       "      <td>Graduate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9070</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.74</td>\n",
       "      <td>Dropout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>9773</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>12.400000</td>\n",
       "      <td>0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-3.12</td>\n",
       "      <td>Graduate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>8014</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>13.9</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.79</td>\n",
       "      <td>Graduate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>9991</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>133.1</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>5</td>\n",
       "      <td>16.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.92</td>\n",
       "      <td>Graduate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>142.0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>14.345000</td>\n",
       "      <td>0</td>\n",
       "      <td>15.5</td>\n",
       "      <td>2.8</td>\n",
       "      <td>-4.06</td>\n",
       "      <td>Graduate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>9254</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>119.0</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>15.5</td>\n",
       "      <td>2.8</td>\n",
       "      <td>-4.06</td>\n",
       "      <td>Dropout</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>9238</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>137.0</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>14.142857</td>\n",
       "      <td>0</td>\n",
       "      <td>16.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.92</td>\n",
       "      <td>Graduate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9238</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>138.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>13.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>8.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3.51</td>\n",
       "      <td>Dropout</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Marital status  Application mode  Application order  Course  \\\n",
       "0               1                17                  5     171   \n",
       "1               1                15                  1    9254   \n",
       "2               1                 1                  5    9070   \n",
       "3               1                17                  2    9773   \n",
       "4               2                39                  1    8014   \n",
       "5               2                39                  1    9991   \n",
       "6               1                 1                  1    9500   \n",
       "7               1                18                  4    9254   \n",
       "8               1                 1                  3    9238   \n",
       "9               1                 1                  1    9238   \n",
       "\n",
       "   Daytime/evening attendance  Previous qualification  \\\n",
       "0                           1                       1   \n",
       "1                           1                       1   \n",
       "2                           1                       1   \n",
       "3                           1                       1   \n",
       "4                           0                       1   \n",
       "5                           0                      19   \n",
       "6                           1                       1   \n",
       "7                           1                       1   \n",
       "8                           1                       1   \n",
       "9                           1                       1   \n",
       "\n",
       "   Previous qualification (grade)  Nacionality  Mother's qualification  \\\n",
       "0                           122.0            1                      19   \n",
       "1                           160.0            1                       1   \n",
       "2                           122.0            1                      37   \n",
       "3                           122.0            1                      38   \n",
       "4                           100.0            1                      37   \n",
       "5                           133.1            1                      37   \n",
       "6                           142.0            1                      19   \n",
       "7                           119.0            1                      37   \n",
       "8                           137.0           62                       1   \n",
       "9                           138.0            1                       1   \n",
       "\n",
       "   Father's qualification  ...  Curricular units 2nd sem (credited)  \\\n",
       "0                      12  ...                                    0   \n",
       "1                       3  ...                                    0   \n",
       "2                      37  ...                                    0   \n",
       "3                      37  ...                                    0   \n",
       "4                      38  ...                                    0   \n",
       "5                      37  ...                                    0   \n",
       "6                      38  ...                                    0   \n",
       "7                      37  ...                                    0   \n",
       "8                       1  ...                                    0   \n",
       "9                      19  ...                                    0   \n",
       "\n",
       "   Curricular units 2nd sem (enrolled)  \\\n",
       "0                                    0   \n",
       "1                                    6   \n",
       "2                                    6   \n",
       "3                                    6   \n",
       "4                                    6   \n",
       "5                                    5   \n",
       "6                                    8   \n",
       "7                                    5   \n",
       "8                                    6   \n",
       "9                                    6   \n",
       "\n",
       "   Curricular units 2nd sem (evaluations)  \\\n",
       "0                                       0   \n",
       "1                                       6   \n",
       "2                                       0   \n",
       "3                                      10   \n",
       "4                                       6   \n",
       "5                                      17   \n",
       "6                                       8   \n",
       "7                                       5   \n",
       "8                                       7   \n",
       "9                                      14   \n",
       "\n",
       "   Curricular units 2nd sem (approved)  Curricular units 2nd sem (grade)  \\\n",
       "0                                    0                          0.000000   \n",
       "1                                    6                         13.666667   \n",
       "2                                    0                          0.000000   \n",
       "3                                    5                         12.400000   \n",
       "4                                    6                         13.000000   \n",
       "5                                    5                         11.500000   \n",
       "6                                    8                         14.345000   \n",
       "7                                    0                          0.000000   \n",
       "8                                    6                         14.142857   \n",
       "9                                    2                         13.500000   \n",
       "\n",
       "   Curricular units 2nd sem (without evaluations)  Unemployment rate  \\\n",
       "0                                               0               10.8   \n",
       "1                                               0               13.9   \n",
       "2                                               0               10.8   \n",
       "3                                               0                9.4   \n",
       "4                                               0               13.9   \n",
       "5                                               5               16.2   \n",
       "6                                               0               15.5   \n",
       "7                                               0               15.5   \n",
       "8                                               0               16.2   \n",
       "9                                               0                8.9   \n",
       "\n",
       "   Inflation rate   GDP    Target  \n",
       "0             1.4  1.74   Dropout  \n",
       "1            -0.3  0.79  Graduate  \n",
       "2             1.4  1.74   Dropout  \n",
       "3            -0.8 -3.12  Graduate  \n",
       "4            -0.3  0.79  Graduate  \n",
       "5             0.3 -0.92  Graduate  \n",
       "6             2.8 -4.06  Graduate  \n",
       "7             2.8 -4.06   Dropout  \n",
       "8             0.3 -0.92  Graduate  \n",
       "9             1.4  3.51   Dropout  \n",
       "\n",
       "[10 rows x 37 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#dataset import\n",
    "dataset = pd.read_csv('data.csv') #You need to change #directory accordingly\n",
    "dataset.head(10) #Return 10 rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Graduate'],\n",
       "       ['Graduate'],\n",
       "       ['Dropout'],\n",
       "       ['Graduate'],\n",
       "       ['Graduate'],\n",
       "       ['Dropout'],\n",
       "       ['Enrolled'],\n",
       "       ['Graduate'],\n",
       "       ['Graduate'],\n",
       "       ['Enrolled'],\n",
       "       ['Graduate'],\n",
       "       ['Enrolled'],\n",
       "       ['Graduate'],\n",
       "       ['Graduate'],\n",
       "       ['Graduate'],\n",
       "       ['Enrolled'],\n",
       "       ['Graduate'],\n",
       "       ['Enrolled'],\n",
       "       ['Graduate'],\n",
       "       ['Enrolled'],\n",
       "       ['Enrolled'],\n",
       "       ['Graduate'],\n",
       "       ['Enrolled'],\n",
       "       ['Graduate'],\n",
       "       ['Graduate'],\n",
       "       ['Dropout'],\n",
       "       ['Dropout'],\n",
       "       ['Dropout'],\n",
       "       ['Graduate'],\n",
       "       ['Dropout']], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Changing pandas dataframe to numpy array\n",
    "X = dataset.iloc[:,:36].values\n",
    "y = dataset.iloc[:,36:37].values\n",
    "y[10:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:98: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_Y = encoder.transform(y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "dummy_y[10:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graduate: 001; Dropout: 100; Enrolled = 010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,dummy_y,test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dependencies\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "# Neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(units=36, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(units=12, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(units=3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.9634 - accuracy: 0.5730 - val_loss: 0.7915 - val_accuracy: 0.6569\n",
      "Epoch 2/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.7646 - accuracy: 0.6923 - val_loss: 0.7138 - val_accuracy: 0.6930\n",
      "Epoch 3/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.6952 - accuracy: 0.7192 - val_loss: 0.6856 - val_accuracy: 0.7201\n",
      "Epoch 4/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.6582 - accuracy: 0.7360 - val_loss: 0.6712 - val_accuracy: 0.7314\n",
      "Epoch 5/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.6295 - accuracy: 0.7476 - val_loss: 0.6630 - val_accuracy: 0.7427\n",
      "Epoch 6/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.6161 - accuracy: 0.7511 - val_loss: 0.6585 - val_accuracy: 0.7517\n",
      "Epoch 7/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.6011 - accuracy: 0.7498 - val_loss: 0.6565 - val_accuracy: 0.7494\n",
      "Epoch 8/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5994 - accuracy: 0.7599 - val_loss: 0.6499 - val_accuracy: 0.7585\n",
      "Epoch 9/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5897 - accuracy: 0.7589 - val_loss: 0.6498 - val_accuracy: 0.7652\n",
      "Epoch 10/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5899 - accuracy: 0.7621 - val_loss: 0.6442 - val_accuracy: 0.7562\n",
      "Epoch 11/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5815 - accuracy: 0.7629 - val_loss: 0.6479 - val_accuracy: 0.7517\n",
      "Epoch 12/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5724 - accuracy: 0.7737 - val_loss: 0.6445 - val_accuracy: 0.7585\n",
      "Epoch 13/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5801 - accuracy: 0.7697 - val_loss: 0.6384 - val_accuracy: 0.7698\n",
      "Epoch 14/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.5622 - accuracy: 0.7734 - val_loss: 0.6428 - val_accuracy: 0.7720\n",
      "Epoch 15/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5656 - accuracy: 0.7757 - val_loss: 0.6441 - val_accuracy: 0.7698\n",
      "Epoch 16/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5645 - accuracy: 0.7737 - val_loss: 0.6500 - val_accuracy: 0.7698\n",
      "Epoch 17/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.5564 - accuracy: 0.7724 - val_loss: 0.6435 - val_accuracy: 0.7765\n",
      "Epoch 18/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.5533 - accuracy: 0.7767 - val_loss: 0.6457 - val_accuracy: 0.7720\n",
      "Epoch 19/600\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 0.5594 - accuracy: 0.7742 - val_loss: 0.6487 - val_accuracy: 0.7743\n",
      "Epoch 20/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.5405 - accuracy: 0.7797 - val_loss: 0.6452 - val_accuracy: 0.7810\n",
      "Epoch 21/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.5474 - accuracy: 0.7812 - val_loss: 0.6447 - val_accuracy: 0.7720\n",
      "Epoch 22/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.5438 - accuracy: 0.7769 - val_loss: 0.6514 - val_accuracy: 0.7720\n",
      "Epoch 23/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5461 - accuracy: 0.7772 - val_loss: 0.6400 - val_accuracy: 0.7720\n",
      "Epoch 24/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.5379 - accuracy: 0.7830 - val_loss: 0.6409 - val_accuracy: 0.7743\n",
      "Epoch 25/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.5404 - accuracy: 0.7815 - val_loss: 0.6441 - val_accuracy: 0.7810\n",
      "Epoch 26/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.5370 - accuracy: 0.7807 - val_loss: 0.6594 - val_accuracy: 0.7607\n",
      "Epoch 27/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.5360 - accuracy: 0.7825 - val_loss: 0.6507 - val_accuracy: 0.7743\n",
      "Epoch 28/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.5393 - accuracy: 0.7812 - val_loss: 0.6488 - val_accuracy: 0.7720\n",
      "Epoch 29/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.5320 - accuracy: 0.7860 - val_loss: 0.6470 - val_accuracy: 0.7810\n",
      "Epoch 30/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.5234 - accuracy: 0.7880 - val_loss: 0.6539 - val_accuracy: 0.7788\n",
      "Epoch 31/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.5284 - accuracy: 0.7802 - val_loss: 0.6606 - val_accuracy: 0.7675\n",
      "Epoch 32/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.5229 - accuracy: 0.7865 - val_loss: 0.6537 - val_accuracy: 0.7810\n",
      "Epoch 33/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.5321 - accuracy: 0.7880 - val_loss: 0.6655 - val_accuracy: 0.7562\n",
      "Epoch 34/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.5184 - accuracy: 0.7877 - val_loss: 0.6661 - val_accuracy: 0.7607\n",
      "Epoch 35/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.5221 - accuracy: 0.7810 - val_loss: 0.6651 - val_accuracy: 0.7630\n",
      "Epoch 36/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5243 - accuracy: 0.7900 - val_loss: 0.6602 - val_accuracy: 0.7720\n",
      "Epoch 37/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5248 - accuracy: 0.7915 - val_loss: 0.6649 - val_accuracy: 0.7562\n",
      "Epoch 38/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.5183 - accuracy: 0.7870 - val_loss: 0.6754 - val_accuracy: 0.7585\n",
      "Epoch 39/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.5198 - accuracy: 0.7890 - val_loss: 0.6652 - val_accuracy: 0.7720\n",
      "Epoch 40/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5137 - accuracy: 0.7930 - val_loss: 0.6769 - val_accuracy: 0.7675\n",
      "Epoch 41/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5179 - accuracy: 0.7870 - val_loss: 0.6848 - val_accuracy: 0.7675\n",
      "Epoch 42/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.5120 - accuracy: 0.7945 - val_loss: 0.6844 - val_accuracy: 0.7585\n",
      "Epoch 43/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5117 - accuracy: 0.7852 - val_loss: 0.6756 - val_accuracy: 0.7765\n",
      "Epoch 44/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5200 - accuracy: 0.7805 - val_loss: 0.6920 - val_accuracy: 0.7540\n",
      "Epoch 45/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5131 - accuracy: 0.7955 - val_loss: 0.6872 - val_accuracy: 0.7652\n",
      "Epoch 46/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5076 - accuracy: 0.7940 - val_loss: 0.6865 - val_accuracy: 0.7675\n",
      "Epoch 47/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5101 - accuracy: 0.8006 - val_loss: 0.6973 - val_accuracy: 0.7630\n",
      "Epoch 48/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5115 - accuracy: 0.7895 - val_loss: 0.6898 - val_accuracy: 0.7630\n",
      "Epoch 49/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5055 - accuracy: 0.7925 - val_loss: 0.6862 - val_accuracy: 0.7607\n",
      "Epoch 50/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5087 - accuracy: 0.7940 - val_loss: 0.6934 - val_accuracy: 0.7652\n",
      "Epoch 51/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5006 - accuracy: 0.7933 - val_loss: 0.6954 - val_accuracy: 0.7652\n",
      "Epoch 52/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5003 - accuracy: 0.7925 - val_loss: 0.7024 - val_accuracy: 0.7562\n",
      "Epoch 53/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5053 - accuracy: 0.7973 - val_loss: 0.6857 - val_accuracy: 0.7494\n",
      "Epoch 54/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5054 - accuracy: 0.7898 - val_loss: 0.7016 - val_accuracy: 0.7517\n",
      "Epoch 55/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5002 - accuracy: 0.7895 - val_loss: 0.6962 - val_accuracy: 0.7449\n",
      "Epoch 56/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4938 - accuracy: 0.8018 - val_loss: 0.6958 - val_accuracy: 0.7517\n",
      "Epoch 57/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4961 - accuracy: 0.7950 - val_loss: 0.7034 - val_accuracy: 0.7562\n",
      "Epoch 58/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.5031 - accuracy: 0.7970 - val_loss: 0.7010 - val_accuracy: 0.7427\n",
      "Epoch 59/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4944 - accuracy: 0.7980 - val_loss: 0.6995 - val_accuracy: 0.7585\n",
      "Epoch 60/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4962 - accuracy: 0.8003 - val_loss: 0.7087 - val_accuracy: 0.7494\n",
      "Epoch 61/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4958 - accuracy: 0.7925 - val_loss: 0.7047 - val_accuracy: 0.7427\n",
      "Epoch 62/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4979 - accuracy: 0.7898 - val_loss: 0.7116 - val_accuracy: 0.7517\n",
      "Epoch 63/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4924 - accuracy: 0.7988 - val_loss: 0.7178 - val_accuracy: 0.7449\n",
      "Epoch 64/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4930 - accuracy: 0.7975 - val_loss: 0.7118 - val_accuracy: 0.7427\n",
      "Epoch 65/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4949 - accuracy: 0.7945 - val_loss: 0.7174 - val_accuracy: 0.7517\n",
      "Epoch 66/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4874 - accuracy: 0.7940 - val_loss: 0.7190 - val_accuracy: 0.7472\n",
      "Epoch 67/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4876 - accuracy: 0.7995 - val_loss: 0.7113 - val_accuracy: 0.7449\n",
      "Epoch 68/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4827 - accuracy: 0.8018 - val_loss: 0.7120 - val_accuracy: 0.7449\n",
      "Epoch 69/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4846 - accuracy: 0.8098 - val_loss: 0.7104 - val_accuracy: 0.7540\n",
      "Epoch 70/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4965 - accuracy: 0.7978 - val_loss: 0.7097 - val_accuracy: 0.7427\n",
      "Epoch 71/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4824 - accuracy: 0.7988 - val_loss: 0.7201 - val_accuracy: 0.7562\n",
      "Epoch 72/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4860 - accuracy: 0.8028 - val_loss: 0.7193 - val_accuracy: 0.7449\n",
      "Epoch 73/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4868 - accuracy: 0.8073 - val_loss: 0.7112 - val_accuracy: 0.7540\n",
      "Epoch 74/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4767 - accuracy: 0.8051 - val_loss: 0.7253 - val_accuracy: 0.7540\n",
      "Epoch 75/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4770 - accuracy: 0.8036 - val_loss: 0.7203 - val_accuracy: 0.7540\n",
      "Epoch 76/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.4839 - accuracy: 0.7995 - val_loss: 0.7208 - val_accuracy: 0.7472\n",
      "Epoch 77/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.4852 - accuracy: 0.8023 - val_loss: 0.7144 - val_accuracy: 0.7517\n",
      "Epoch 78/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4769 - accuracy: 0.8006 - val_loss: 0.7143 - val_accuracy: 0.7540\n",
      "Epoch 79/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4775 - accuracy: 0.8018 - val_loss: 0.7233 - val_accuracy: 0.7540\n",
      "Epoch 80/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4890 - accuracy: 0.7998 - val_loss: 0.7146 - val_accuracy: 0.7675\n",
      "Epoch 81/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4882 - accuracy: 0.7958 - val_loss: 0.7136 - val_accuracy: 0.7585\n",
      "Epoch 82/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4756 - accuracy: 0.8038 - val_loss: 0.7289 - val_accuracy: 0.7562\n",
      "Epoch 83/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4695 - accuracy: 0.8031 - val_loss: 0.7261 - val_accuracy: 0.7562\n",
      "Epoch 84/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4730 - accuracy: 0.8048 - val_loss: 0.7325 - val_accuracy: 0.7540\n",
      "Epoch 85/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4723 - accuracy: 0.8063 - val_loss: 0.7252 - val_accuracy: 0.7562\n",
      "Epoch 86/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4741 - accuracy: 0.8043 - val_loss: 0.7469 - val_accuracy: 0.7472\n",
      "Epoch 87/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4737 - accuracy: 0.8031 - val_loss: 0.7454 - val_accuracy: 0.7494\n",
      "Epoch 88/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4619 - accuracy: 0.8076 - val_loss: 0.7456 - val_accuracy: 0.7517\n",
      "Epoch 89/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4792 - accuracy: 0.7988 - val_loss: 0.7500 - val_accuracy: 0.7449\n",
      "Epoch 90/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4811 - accuracy: 0.8053 - val_loss: 0.7379 - val_accuracy: 0.7494\n",
      "Epoch 91/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4749 - accuracy: 0.8041 - val_loss: 0.7398 - val_accuracy: 0.7494\n",
      "Epoch 92/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4740 - accuracy: 0.8051 - val_loss: 0.7418 - val_accuracy: 0.7472\n",
      "Epoch 93/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4731 - accuracy: 0.8033 - val_loss: 0.7501 - val_accuracy: 0.7517\n",
      "Epoch 94/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4757 - accuracy: 0.8033 - val_loss: 0.7464 - val_accuracy: 0.7427\n",
      "Epoch 95/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4697 - accuracy: 0.8078 - val_loss: 0.7397 - val_accuracy: 0.7494\n",
      "Epoch 96/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4679 - accuracy: 0.8081 - val_loss: 0.7593 - val_accuracy: 0.7427\n",
      "Epoch 97/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4740 - accuracy: 0.8091 - val_loss: 0.7514 - val_accuracy: 0.7427\n",
      "Epoch 98/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4685 - accuracy: 0.8038 - val_loss: 0.7451 - val_accuracy: 0.7540\n",
      "Epoch 99/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4649 - accuracy: 0.8036 - val_loss: 0.7451 - val_accuracy: 0.7472\n",
      "Epoch 100/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4626 - accuracy: 0.8053 - val_loss: 0.7458 - val_accuracy: 0.7472\n",
      "Epoch 101/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4695 - accuracy: 0.8073 - val_loss: 0.7391 - val_accuracy: 0.7585\n",
      "Epoch 102/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4654 - accuracy: 0.8096 - val_loss: 0.7485 - val_accuracy: 0.7359\n",
      "Epoch 103/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4723 - accuracy: 0.8033 - val_loss: 0.7522 - val_accuracy: 0.7381\n",
      "Epoch 104/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4595 - accuracy: 0.8073 - val_loss: 0.7501 - val_accuracy: 0.7449\n",
      "Epoch 105/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4630 - accuracy: 0.8101 - val_loss: 0.7546 - val_accuracy: 0.7449\n",
      "Epoch 106/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4597 - accuracy: 0.8078 - val_loss: 0.7589 - val_accuracy: 0.7472\n",
      "Epoch 107/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4636 - accuracy: 0.8078 - val_loss: 0.7570 - val_accuracy: 0.7630\n",
      "Epoch 108/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4602 - accuracy: 0.8051 - val_loss: 0.7519 - val_accuracy: 0.7472\n",
      "Epoch 109/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4659 - accuracy: 0.8058 - val_loss: 0.7472 - val_accuracy: 0.7517\n",
      "Epoch 110/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4712 - accuracy: 0.8056 - val_loss: 0.7512 - val_accuracy: 0.7427\n",
      "Epoch 111/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4627 - accuracy: 0.8129 - val_loss: 0.7650 - val_accuracy: 0.7472\n",
      "Epoch 112/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4617 - accuracy: 0.8106 - val_loss: 0.7633 - val_accuracy: 0.7427\n",
      "Epoch 113/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4535 - accuracy: 0.8021 - val_loss: 0.7732 - val_accuracy: 0.7449\n",
      "Epoch 114/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4543 - accuracy: 0.8088 - val_loss: 0.7657 - val_accuracy: 0.7494\n",
      "Epoch 115/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4571 - accuracy: 0.8139 - val_loss: 0.7640 - val_accuracy: 0.7472\n",
      "Epoch 116/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4521 - accuracy: 0.8139 - val_loss: 0.7712 - val_accuracy: 0.7540\n",
      "Epoch 117/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4635 - accuracy: 0.8028 - val_loss: 0.7693 - val_accuracy: 0.7472\n",
      "Epoch 118/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4471 - accuracy: 0.8194 - val_loss: 0.7851 - val_accuracy: 0.7472\n",
      "Epoch 119/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4592 - accuracy: 0.8098 - val_loss: 0.7700 - val_accuracy: 0.7472\n",
      "Epoch 120/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4589 - accuracy: 0.8101 - val_loss: 0.7609 - val_accuracy: 0.7517\n",
      "Epoch 121/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4494 - accuracy: 0.8191 - val_loss: 0.7738 - val_accuracy: 0.7449\n",
      "Epoch 122/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4651 - accuracy: 0.8093 - val_loss: 0.7568 - val_accuracy: 0.7404\n",
      "Epoch 123/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4552 - accuracy: 0.8126 - val_loss: 0.7623 - val_accuracy: 0.7494\n",
      "Epoch 124/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4545 - accuracy: 0.8121 - val_loss: 0.7672 - val_accuracy: 0.7449\n",
      "Epoch 125/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4529 - accuracy: 0.8136 - val_loss: 0.7657 - val_accuracy: 0.7540\n",
      "Epoch 126/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4499 - accuracy: 0.8119 - val_loss: 0.7755 - val_accuracy: 0.7472\n",
      "Epoch 127/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4485 - accuracy: 0.8129 - val_loss: 0.7818 - val_accuracy: 0.7562\n",
      "Epoch 128/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4442 - accuracy: 0.8179 - val_loss: 0.7829 - val_accuracy: 0.7585\n",
      "Epoch 129/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4459 - accuracy: 0.8136 - val_loss: 0.7802 - val_accuracy: 0.7562\n",
      "Epoch 130/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.4491 - accuracy: 0.8119 - val_loss: 0.7799 - val_accuracy: 0.7562\n",
      "Epoch 131/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.4527 - accuracy: 0.8126 - val_loss: 0.7844 - val_accuracy: 0.7517\n",
      "Epoch 132/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4593 - accuracy: 0.8091 - val_loss: 0.7860 - val_accuracy: 0.7449\n",
      "Epoch 133/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4590 - accuracy: 0.8103 - val_loss: 0.7961 - val_accuracy: 0.7472\n",
      "Epoch 134/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.4518 - accuracy: 0.8164 - val_loss: 0.7751 - val_accuracy: 0.7562\n",
      "Epoch 135/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.4527 - accuracy: 0.8189 - val_loss: 0.7852 - val_accuracy: 0.7562\n",
      "Epoch 136/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.4516 - accuracy: 0.8164 - val_loss: 0.7837 - val_accuracy: 0.7585\n",
      "Epoch 137/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4464 - accuracy: 0.8151 - val_loss: 0.7944 - val_accuracy: 0.7517\n",
      "Epoch 138/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.4511 - accuracy: 0.8189 - val_loss: 0.7951 - val_accuracy: 0.7472\n",
      "Epoch 139/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.4436 - accuracy: 0.8181 - val_loss: 0.8061 - val_accuracy: 0.7517\n",
      "Epoch 140/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.4439 - accuracy: 0.8151 - val_loss: 0.8040 - val_accuracy: 0.7494\n",
      "Epoch 141/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.4463 - accuracy: 0.8169 - val_loss: 0.8024 - val_accuracy: 0.7494\n",
      "Epoch 142/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4367 - accuracy: 0.8217 - val_loss: 0.7955 - val_accuracy: 0.7540\n",
      "Epoch 143/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4446 - accuracy: 0.8171 - val_loss: 0.7988 - val_accuracy: 0.7494\n",
      "Epoch 144/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4444 - accuracy: 0.8146 - val_loss: 0.7868 - val_accuracy: 0.7562\n",
      "Epoch 145/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4459 - accuracy: 0.8191 - val_loss: 0.7931 - val_accuracy: 0.7607\n",
      "Epoch 146/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4451 - accuracy: 0.8169 - val_loss: 0.8122 - val_accuracy: 0.7494\n",
      "Epoch 147/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4550 - accuracy: 0.8101 - val_loss: 0.7947 - val_accuracy: 0.7562\n",
      "Epoch 148/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4425 - accuracy: 0.8191 - val_loss: 0.8085 - val_accuracy: 0.7607\n",
      "Epoch 149/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4391 - accuracy: 0.8189 - val_loss: 0.8029 - val_accuracy: 0.7517\n",
      "Epoch 150/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4485 - accuracy: 0.8149 - val_loss: 0.7996 - val_accuracy: 0.7585\n",
      "Epoch 151/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.4394 - accuracy: 0.8206 - val_loss: 0.7815 - val_accuracy: 0.7562\n",
      "Epoch 152/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.4430 - accuracy: 0.8179 - val_loss: 0.7918 - val_accuracy: 0.7607\n",
      "Epoch 153/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4468 - accuracy: 0.8169 - val_loss: 0.8018 - val_accuracy: 0.7562\n",
      "Epoch 154/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4417 - accuracy: 0.8171 - val_loss: 0.8062 - val_accuracy: 0.7540\n",
      "Epoch 155/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4529 - accuracy: 0.8156 - val_loss: 0.7945 - val_accuracy: 0.7449\n",
      "Epoch 156/600\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 0.4394 - accuracy: 0.8176 - val_loss: 0.8001 - val_accuracy: 0.7517\n",
      "Epoch 157/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.4472 - accuracy: 0.8159 - val_loss: 0.7881 - val_accuracy: 0.7540\n",
      "Epoch 158/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.4431 - accuracy: 0.8106 - val_loss: 0.7998 - val_accuracy: 0.7494\n",
      "Epoch 159/600\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 0.4406 - accuracy: 0.8159 - val_loss: 0.7960 - val_accuracy: 0.7585\n",
      "Epoch 160/600\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.4436 - accuracy: 0.8134 - val_loss: 0.8044 - val_accuracy: 0.7607\n",
      "Epoch 161/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.4411 - accuracy: 0.8184 - val_loss: 0.8122 - val_accuracy: 0.7562\n",
      "Epoch 162/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.4401 - accuracy: 0.8209 - val_loss: 0.8209 - val_accuracy: 0.7472\n",
      "Epoch 163/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.4413 - accuracy: 0.8184 - val_loss: 0.8161 - val_accuracy: 0.7540\n",
      "Epoch 164/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.4393 - accuracy: 0.8204 - val_loss: 0.8021 - val_accuracy: 0.7562\n",
      "Epoch 165/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.4411 - accuracy: 0.8186 - val_loss: 0.8028 - val_accuracy: 0.7562\n",
      "Epoch 166/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.4380 - accuracy: 0.8146 - val_loss: 0.8086 - val_accuracy: 0.7562\n",
      "Epoch 167/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.4304 - accuracy: 0.8244 - val_loss: 0.8053 - val_accuracy: 0.7607\n",
      "Epoch 168/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.4343 - accuracy: 0.8204 - val_loss: 0.8185 - val_accuracy: 0.7562\n",
      "Epoch 169/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.4327 - accuracy: 0.8184 - val_loss: 0.8134 - val_accuracy: 0.7562\n",
      "Epoch 170/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.4353 - accuracy: 0.8073 - val_loss: 0.7959 - val_accuracy: 0.7540\n",
      "Epoch 171/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.4308 - accuracy: 0.8222 - val_loss: 0.7880 - val_accuracy: 0.7562\n",
      "Epoch 172/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.4389 - accuracy: 0.8237 - val_loss: 0.8089 - val_accuracy: 0.7562\n",
      "Epoch 173/600\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.4259 - accuracy: 0.8212 - val_loss: 0.8179 - val_accuracy: 0.7562\n",
      "Epoch 174/600\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 0.4348 - accuracy: 0.8204 - val_loss: 0.8135 - val_accuracy: 0.7607\n",
      "Epoch 175/600\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 0.4493 - accuracy: 0.8103 - val_loss: 0.8151 - val_accuracy: 0.7607\n",
      "Epoch 176/600\n",
      " 21/125 [====>.........................] - ETA: 0s - loss: 0.4287 - accuracy: 0.8304"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\STUDY\\20212\\AI\\Project-AI\\Part2\\Predict-Raw-Data.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/STUDY/20212/AI/Project-AI/Part2/Predict-Raw-Data.ipynb#ch0000009?line=0'>1</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X_train,y_train,epochs\u001b[39m=\u001b[39;49m\u001b[39m600\u001b[39;49m,validation_data\u001b[39m=\u001b[39;49m(X_test,y_test))\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training.py:1413\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1411\u001b[0m   context\u001b[39m.\u001b[39masync_wait()\n\u001b[0;32m   1412\u001b[0m logs \u001b[39m=\u001b[39m tmp_logs  \u001b[39m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[1;32m-> 1413\u001b[0m end_step \u001b[39m=\u001b[39m step \u001b[39m+\u001b[39m data_handler\u001b[39m.\u001b[39;49mstep_increment\n\u001b[0;32m   1414\u001b[0m callbacks\u001b[39m.\u001b[39mon_train_batch_end(end_step, logs)\n\u001b[0;32m   1415\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\data_adapter.py:1268\u001b[0m, in \u001b[0;36mDataHandler.step_increment\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1265\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m steps_remaining\n\u001b[0;32m   1266\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution\u001b[39m.\u001b[39massign(original_spe)\n\u001b[1;32m-> 1268\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m   1269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_increment\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   1270\u001b[0m   \u001b[39m\"\"\"The number to increment the step for `on_batch_end` methods.\"\"\"\u001b[39;00m\n\u001b[0;32m   1271\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_step_increment\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train,epochs=600,validation_data=(X_test,y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d591c6e422414675974e227c13f5382000c440fedd3c5006ef2be5d887f0ba7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
